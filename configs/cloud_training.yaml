# Cloud-optimized training configuration
cloud:
  instance_type: "rtx4090"
  storage_path: "/workspace"
  checkpoint_interval: 100_000_000  # Every 100M tokens
  max_checkpoints: 10

stage1:
  # Text pretrain - 2B tokens
  dataset_path: "data/tokenized/cloud/stage1.bin"
  idx_path: "data/tokenized/cloud/stage1.idx"
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 4000
  seq_len: 4096
  micro_batch_size: 8
  gradient_accumulation: 16
  effective_batch_size: 524288  # 4096 * 4 * 32
  precision: bf16
  log_interval: 20
  eval_interval: 500
  checkpoint_dir: "artifacts/checkpoints/cloud/stage1"
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 2.5e-4
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  lr_schedule:
    type: "cosine"
    warmup_steps: 2000
    min_lr: 2.5e-5
  grad_clip: 1.0
  target_tokens: 2_000_000_000

stage2:
  # Text pretrain - 5B tokens
  dataset_path: "data/tokenized/cloud/stage2.bin"
  idx_path: "data/tokenized/cloud/stage2.idx"
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 10000
  seq_len: 4096
  micro_batch_size: 8
  gradient_accumulation: 16
  effective_batch_size: 524288
  precision: bf16
  log_interval: 20
  eval_interval: 500
  checkpoint_dir: "artifacts/checkpoints/cloud/stage2"
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 2.0e-4  # Slightly lower LR for stage 2
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  lr_schedule:
    type: "cosine"
    warmup_steps: 2000
    min_lr: 2.0e-5
  grad_clip: 1.0
  target_tokens: 5_000_000_000

stage3:
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 1000
  seq_len: 4096
  micro_batch_size: 4
  gradient_accumulation: 32
  precision: bf16
  log_interval: 20
  eval_interval: 200
  checkpoint_dir: "artifacts/checkpoints/cloud/stage3"
  grad_clip: 1.0
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 1.5e-4
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  vision_shards:
    - "/workspace/datasets/vision/stage3/shard-{000000..000100}.tar"
  shuffle_buffer: 1024
  image_token: "<image>"
  adapter:
    fusion: "qformer"
    num_queries: 64
    train_backbone: false
    image_size: 224

stage4:
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 1500
  seq_len: 4096
  micro_batch_size: 4
  gradient_accumulation: 32
  precision: bf16
  log_interval: 20
  eval_interval: 200
  checkpoint_dir: "artifacts/checkpoints/cloud/stage4"
  grad_clip: 1.0
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 1.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  vision_shards:
    - "/workspace/datasets/vision/stage4/shard-{000000..000150}.tar"
  shuffle_buffer: 1024
  image_token: "<image>"
  adapter:
    fusion: "qformer"
    num_queries: 64
    train_backbone: true
    image_size: 224

trainer:
  device: cuda
  use_compile: true
  compile_mode: "reduce-overhead"
  precision: bf16
  gradient_checkpointing: true
  activation_recompute: true
  dataloader_workers: 24
  pin_memory: true
  persistent_workers: true
  env:
    PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
    TORCH_ALLOW_TF32_CUBLAS: "1"
    TORCH_BACKENDS_MATMUL_ALLOW_TF32: "1"
    TORCHINDUCTOR_MAX_AUTOTUNE: "1"
  checkpoint:
    directory: "artifacts/checkpoints/cloud"
    keep_last: 10
    save_optimizer: true
    save_scheduler: true

monitoring:
  wandb:
    project: "fustercluck-cloud"
    entity: null
    tags: ["cloud", "pretrain", "rtx4090"]
  tensorboard:
    log_dir: "logs/tensorboard"
  metrics:
    - "loss"
    - "perplexity"
    - "tokens_per_second"
    - "memory_usage"
    - "gpu_utilization"
