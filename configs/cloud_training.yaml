# Cloud-optimized training configuration
cloud:
  instance_type: "rtx4090"
  storage_path: "/workspace"
  checkpoint_interval: 100_000_000  # Every 100M tokens
  max_checkpoints: 10

stage1:
  # Text pretrain - 2B tokens
  dataset_path: "data/tokenized/cloud/stage1.bin"
  idx_path: "data/tokenized/cloud/stage1.idx"
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 4000
  seq_len: 4096
  micro_batch_size: 8
  gradient_accumulation: 32
  effective_batch_size: 1048576  # 4096 * 8 * 32
  precision: bf16
  log_interval: 20
  eval_interval: 100
  checkpoint_dir: "artifacts/checkpoints/cloud/stage1"
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 2.5e-4
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  lr_schedule:
    type: "cosine"
    warmup_steps: 2000
    min_lr: 2.5e-5
  grad_clip: 1.0
  target_tokens: 2_000_000_000

stage2:
  # Text pretrain - 5B tokens
  dataset_path: "data/tokenized/cloud/stage2.bin"
  idx_path: "data/tokenized/cloud/stage2.idx"
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 10000
  seq_len: 4096
  micro_batch_size: 8
  gradient_accumulation: 32
  effective_batch_size: 1048576
  precision: bf16
  log_interval: 20
  eval_interval: 100
  checkpoint_dir: "artifacts/checkpoints/cloud/stage2"
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 2.0e-4  # Slightly lower LR for stage 2
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  lr_schedule:
    type: "cosine"
    warmup_steps: 2000
    min_lr: 2.0e-5
  grad_clip: 1.0
  target_tokens: 5_000_000_000

stage3:
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 1000
  seq_len: 2048
  micro_batch_size: 4
  gradient_accumulation: 32
  precision: bf16
  log_interval: 20
  eval_interval: 200
  checkpoint_dir: "artifacts/checkpoints/cloud/stage3"
  grad_clip: 1.0
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 1.5e-4
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  vision_shards:
    - "${cloud.storage_path}/datasets/vision/stage3/shard-{000000..000511}.tar"
  shuffle_buffer: 1024
  image_token: "<image>"
  adapter:
    fusion: "qformer"
    num_queries: 64
    train_backbone: false
    image_size: 224

stage4:
  tokenizer_path: "artifacts/tokenizer/fustercluck.model"
  max_steps: 1500
  seq_len: 2048
  micro_batch_size: 4
  gradient_accumulation: 32
  precision: bf16
  log_interval: 20
  eval_interval: 200
  checkpoint_dir: "artifacts/checkpoints/cloud/stage4"
  grad_clip: 1.0
  model_dim: 1024
  model_layers: 24
  model_heads: 16
  model_kv_heads: 4
  mlp_ratio: 4.0
  rope_theta: 10000
  dropout: 0.0
  optimizer:
    lr: 1.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.1
    eps: 1e-8
  vision_shards:
    - "${cloud.storage_path}/datasets/vision/stage4/shard-{000000..001023}.tar"
  shuffle_buffer: 1024
  image_token: "<image>"
  adapter:
    fusion: "qformer"
    num_queries: 64
    train_backbone: true
    image_size: 224

trainer:
  device: cuda
  use_compile: false
  compile_mode: "reduce-overhead"
  precision: bf16
  gradient_checkpointing: true
  activation_recompute: true
  dataloader_workers: 24
  pin_memory: true
  persistent_workers: true
  env:
    PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
    TORCH_ALLOW_TF32_CUBLAS: "1"
    TORCH_BACKENDS_MATMUL_ALLOW_TF32: "1"
    TORCHINDUCTOR_MAX_AUTOTUNE: "1"
  checkpoint:
    directory: "artifacts/checkpoints/cloud"
    keep_last: 10
    save_optimizer: true
    save_scheduler: true

monitoring:
  wandb:
    project: "fustercluck-cloud"
    entity: null
    tags: ["cloud", "pretrain", "rtx4090"]
  tensorboard:
    log_dir: "logs/tensorboard"
  metrics:
    - "loss"
    - "perplexity"
    - "tokens_per_second"
    - "memory_usage"
    - "gpu_utilization"

data:
  hf_token_env: "HF_TOKEN"
  stage1:
    tokenizer_path: "artifacts/tokenizer/fustercluck.model"
    output_prefix: "data/tokenized/cloud/stage1"
    target_tokens: 2000000000
    minimum_chars: 32
    shuffle_buffer: 65536
    datasets:
      - name: "HuggingFaceFW/fineweb"
        config: "sample-350BT"
        split: "train"
        text_field: "text"
        sample_limit: 4000000
        shuffle_buffer: 30000
      - name: "HuggingFaceFW/fineweb"
        config: "sample-100BT"
        split: "train"
        text_field: "text"
        sample_limit: 3000000
        shuffle_buffer: 30000
      - name: "HuggingFaceFW/fineweb"
        config: "sample-10BT"
        split: "train"
        text_field: "text"
        sample_limit: 2000000
        shuffle_buffer: 20000
  stage2:
    tokenizer_path: "artifacts/tokenizer/fustercluck.model"
    output_prefix: "data/tokenized/cloud/stage2"
    target_tokens: 5000000000
    minimum_chars: 32
    shuffle_buffer: 65536
    datasets:
      - name: "HuggingFaceFW/fineweb"
        config: "sample-350BT"
        split: "train"
        text_field: "text"
        sample_limit: 5000000
        shuffle_buffer: 40000
      - name: "HuggingFaceFW/fineweb"
        config: "sample-100BT"
        split: "train"
        text_field: "text"
        sample_limit: 4000000
        shuffle_buffer: 40000
      - name: "HuggingFaceFW/fineweb"
        config: "sample-10BT"
        split: "train"
        text_field: "text"
        sample_limit: 3000000
        shuffle_buffer: 30000
      - name: "HuggingFaceFW/fineweb"
        config: "CC-MAIN-2024-46"
        split: "train"
        text_field: "text"
        sample_limit: 2500000
        shuffle_buffer: 30000
